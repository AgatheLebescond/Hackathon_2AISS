import os
import streamlit as st

from ingestion.extractor import extract_text_from_file
from ingestion.cleaner import clean_text
from ingestion.processing.splitter import split_text
from ingestion.processing.embedder import embed_chunks
from ingestion.processing.indexer import build_faiss_index, index_chunks
from ingestion.processing.summarizer import generate_summary  # , generate_thematic_summary
from ingestion.newsapi_fetcher import fetch_article_from_url

# === Configuration Streamlit
st.set_page_config(page_title="AI RÃ©sumeur", layout="wide")
st.title("ğŸ§  RÃ©sumeur intelligent d'articles & PDF")

# ---------- Session state ----------
defaults = {
    "raw_text": None,
    "cleaned": None,
    "chunks": None,
    "index": None,
    "summary": None,
    "metadata": {},
    "doc_id": None,
    "auto_summarize": True,  # NEW: toggle to auto-generate summary after extraction
}
for k, v in defaults.items():
    if k not in st.session_state:
        st.session_state[k] = v

# === Helpers ===
def save_uploaded_file(uploaded_file):
    filepath = os.path.join("data/uploads", uploaded_file.name)
    with open(filepath, "wb") as f:
        f.write(uploaded_file.read())
    return filepath

def process_text_pipeline(raw_text):
    cleaned = clean_text(raw_text)
    chunks = split_text(cleaned)
    embeddings = embed_chunks(chunks)
    embedding_dim = embeddings.shape[1]
    index = build_faiss_index(embedding_dim)
    index_chunks(index, embeddings)
    return cleaned, chunks, index

def run_pipeline_and_store(text, doc_id=None, metadata=None):
    cleaned, chunks, index = process_text_pipeline(text)
    st.session_state.raw_text = text
    st.session_state.cleaned = cleaned
    st.session_state.chunks = chunks
    st.session_state.index = index
    st.session_state.doc_id = doc_id or "document"
    st.session_state.metadata = metadata or {}

def display_summary_section(chunks, thematic=False, theme=None):
    """Affichage dâ€™un rÃ©sumÃ© gÃ©nÃ©rÃ© automatiquement"""
    with st.spinner("GÃ©nÃ©ration du rÃ©sumÃ©..."):
        if thematic and theme:
            # summary = generate_thematic_summary(chunks, theme=theme)
            summary = generate_summary(chunks)
            st.subheader(f"ğŸŒ RÃ©sumÃ© thÃ©matique : {theme}")
        else:
            summary = generate_summary(chunks)
            st.subheader("ğŸ“ RÃ©sumÃ© gÃ©nÃ©rÃ©")
    st.success(summary)
    return summary

def display_metadata(metadata):
    st.subheader("â„¹ï¸ MÃ©tadonnÃ©es de lâ€™article")
    st.write(f"ğŸ“° Source : **{metadata.get('source', 'N/A')}**")
    st.write(f"ğŸ•“ Date : {metadata.get('publishedAt', 'N/A')}")
    if metadata.get("image_url"):
        st.image(metadata["image_url"], caption="Image dâ€™illustration")

# === Interface principale ===
tab1, tab2, tab3 = st.tabs(["ğŸ“„ Fichier PDF/DOCX", "ğŸŒ Article en ligne", "ğŸ§¾ RÃ©sultats"])

with tab1:
    st.checkbox(
        "GÃ©nÃ©rer automatiquement un rÃ©sumÃ© aprÃ¨s l'extraction",
        key="auto_summarize",
        help="Si cochÃ©, le rÃ©sumÃ© est produit dÃ¨s que l'article/le fichier est chargÃ©."
    )
    uploaded_file = st.file_uploader("TÃ©lÃ©verser un fichier", type=["pdf", "docx"], key="file_uploader")
    if uploaded_file:
        doc_id = os.path.splitext(uploaded_file.name)[0]
        filepath = save_uploaded_file(uploaded_file)
        text = extract_text_from_file(filepath)
        run_pipeline_and_store(text, doc_id=doc_id, metadata={})
        st.success("Fichier chargÃ©.")
        if st.session_state.auto_summarize and st.session_state.chunks:
            st.session_state.summary = display_summary_section(st.session_state.chunks)

with tab2:
    url = st.text_input("Coller lâ€™URL dâ€™un article dâ€™actualitÃ©", key="url_input")
    if st.button("Extraire lâ€™article depuis lâ€™URL", key="fetch_url") and url:
        try:
            st.markdown("Fetcherâ€¦")
            article_data = fetch_article_from_url(url, api_token="00d1111a7e584642bf066fb39c6746b8")
            text = article_data.get('text', '') or ''
            meta = article_data.get('metadata', {}) or {}
            doc_id = meta.get('title', 'article')
            if not text.strip():
                st.warning("Aucun texte nâ€™a Ã©tÃ© extrait depuis lâ€™URL.")
            run_pipeline_and_store(text, doc_id=doc_id, metadata=meta)
            st.success("Article extrait.")
            if st.session_state.auto_summarize and st.session_state.chunks:
                st.session_state.summary = display_summary_section(st.session_state.chunks)
        except Exception as e:
            st.error(f"Erreur lors de lâ€™extraction : {e}")

    # Boutons de gÃ©nÃ©ration manuelle
    if st.session_state.chunks:
        st.markdown("---")
        st.markdown("## ğŸ¯ Choisir le type de rÃ©sumÃ©")
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ“„ RÃ©sumÃ© classique", key="classic_summary"):
                st.session_state.summary = display_summary_section(st.session_state.chunks)
        with col2:
            theme = st.selectbox(
                "ğŸŒ Choisir un thÃ¨me pour le rÃ©sumÃ© thÃ©matique",
                ["climat", "mobilisation citoyenne", "loi Duplomb"],
                key="theme_select"
            )
            if st.button("ğŸŒ RÃ©sumÃ© thÃ©matique", key="thematic_summary"):
                st.session_state.summary = display_summary_section(
                    st.session_state.chunks, thematic=True, theme=theme
                )

with tab3:
    st.markdown("## ğŸ§¾ RÃ©sultats")
    # RÃ©sumÃ© (toujours affichÃ© s'il existe)
    if st.session_state.summary:
        st.subheader("ğŸ“ RÃ©sumÃ©")
        st.text_area("RÃ©sumÃ© gÃ©nÃ©rÃ©", st.session_state.summary, height=280)
        st.download_button(
            "ğŸ’¾ TÃ©lÃ©charger le rÃ©sumÃ© (.txt)",
            st.session_state.summary,
            file_name=f"{st.session_state.doc_id or 'document'}_resume.txt",
            key="download_summary"
        )
    else:
        st.info("Aucun rÃ©sumÃ© pour lâ€™instant. Importez un contenu et gÃ©nÃ©rez un rÃ©sumÃ©.")

    # MÃ©tadonnÃ©es (si disponibles)
    if st.session_state.metadata:
        st.markdown("---")
        display_metadata(st.session_state.metadata)

    # Diagnostics utiles
    if st.session_state.cleaned or st.session_state.chunks:
        st.markdown("---")
        st.subheader("ğŸ” Diagnostics")
        c1, c2, c3 = st.columns(3)
        if st.session_state.cleaned:
            c1.metric("Mots (texte nettoyÃ©)", len(st.session_state.cleaned.split()))
        if st.session_state.chunks:
            c2.metric("Nombre de chunks", len(st.session_state.chunks))
            # aperÃ§u du 1er chunk
            with st.expander("AperÃ§u du 1er chunk"):
                st.code(st.session_state.chunks[0][:1000])
        if st.session_state.index:
            c3.write("Index FAISS : **OK**")

# Message dâ€™accueil si rien encore
if not (st.session_state.raw_text or st.session_state.chunks or st.session_state.summary):
    st.info("TÃ©lÃ©verse un fichier ou entre une URL pour commencer.")

# Commentaires du code

## 1. Imports

```python
import os
import streamlit as st
os : pour manipuler les chemins de fichiers.
streamlit : bibliothÃ¨que pour crÃ©er une application web interactive avec Python.
from ingestion.extractor import extract_text_from_file
#Fonction personnalisÃ©e pour extraire le texte dâ€™un fichier (PDF ou DOCX).
from ingestion.cleaner import clean_text
#Fonction de nettoyage : supprime les caractÃ¨res inutiles, normalise le texte.
from ingestion.processing.splitter import split_text
#DÃ©coupe le texte long en morceaux plus petits (chunks) pour traitement vectoriel.
from ingestion.processing.embedder import embed_chunks
#GÃ©nÃ¨re des reprÃ©sentations vectorielles (embeddings) Ã  partir des chunks.
from ingestion.processing.indexer import build_faiss_index, index_chunks
#CrÃ©e un index FAISS (recherche sÃ©mantique rapide) et y insÃ¨re les vecteurs.
from ingestion.processing.summarizer import generate_summary
#Fonction qui utilise un modÃ¨le de langage pour rÃ©sumer automatiquement le texte.
from ingestion.newsapi_fetcher import fetch_article_from_url
#Fonction qui rÃ©cupÃ¨re le contenu dâ€™un article en ligne via une API.

2. Configuration de la page Streamlit
st.set_page_config(page_title="AI RÃ©sumeur", layout="wide")
#DÃ©finit le titre et le layout Ã©largi pour lâ€™interface.
st.title("ğŸ§  RÃ©sumeur intelligent d'articles & PDF")
#Affiche le titre principal dans lâ€™interface.

3. Initialisation du session_state
defaults = { ... }
for k, v in defaults.items():
    if k not in st.session_state:
        st.session_state[k] = v
#Ce bloc initialise des variables persistantes pour mÃ©moriser les Ã©tapes entre les actions utilisateur :
texte brut, texte nettoyÃ©, rÃ©sumÃ©, chunks, index, etc.

4. Sauvegarde du fichier uploadÃ©
def save_uploaded_file(uploaded_file):
    ...
#Enregistre localement les fichiers uploadÃ©s dans un dossier data/uploads/.

5. Pipeline de traitement texte
def process_text_pipeline(raw_text):
    ...
#Nettoie le texte,
#DÃ©coupe en chunks,
#GÃ©nÃ¨re les embeddings,
#CrÃ©e et alimente un index FAISS.

6. Stockage des rÃ©sultats dans la session
def run_pipeline_and_store(text, doc_id=None, metadata=None):
    ...
#Stocke tous les rÃ©sultats intermÃ©diaires dans st.session_state pour Ãªtre utilisÃ©s dans les onglets suivants.

7. GÃ©nÃ©ration du rÃ©sumÃ©
def display_summary_section(chunks, thematic=False, theme=None):
    ...
#Affiche un rÃ©sumÃ© automatique (thÃ©matique ou non),
#Utilise la fonction generate_summary() pour crÃ©er un rÃ©sumÃ© textuel Ã  partir des chunks.

8. Affichage des mÃ©tadonnÃ©es
def display_metadata(metadata):
    ...
#Affiche les informations associÃ©es Ã  un article (source, date, image), si disponibles.

9. Interface principale Onglets
tab1, tab2, tab3 = st.tabs(["ğŸ“„ Fichier PDF/DOCX", "ğŸŒ Article en ligne", "ğŸ§¾ RÃ©sultats"])
#Organisation de lâ€™interface en 3 onglets :
tab1 : upload de fichiers,
tab2 : extraction depuis une URL,
tab3 : rÃ©sumÃ© et diagnostic.

10. Onglet 1  Upload de fichier
with tab1:
    ...
#Lâ€™utilisateur tÃ©lÃ©verse un fichier local,
#Le texte est extrait, puis traitÃ© avec le pipeline complet,
#Si la case est cochÃ©e (auto_summarize), le rÃ©sumÃ© est gÃ©nÃ©rÃ© automatiquement.

11. Onglet 2  Extraction dâ€™article via URL
with tab2:
    ...
#Lâ€™utilisateur colle une URL dâ€™article,
#Le texte est extrait via NewsAPI,
#Stockage des rÃ©sultats et affichage du rÃ©sumÃ©,
#Option de choisir un thÃ¨me de rÃ©sumÃ©.

12. Onglet 3  RÃ©sultats
with tab3:
    ...
#Affichage du rÃ©sumÃ© gÃ©nÃ©rÃ© (si disponible),
#TÃ©lÃ©chargement du rÃ©sumÃ© (.txt),
#Affichage des mÃ©tadonnÃ©es,
#Affichage de diagnostics techniques :
#nombre de mots,
#nombre de chunks,
#vÃ©rification FAISS,
#aperÃ§u du premier chunk.

13. Message dâ€™accueil
if not (st.session_state.raw_text or st.session_state.chunks or st.session_state.summary):
    st.info("TÃ©lÃ©verse un fichier ou entre une URL pour commencer.")
#Message affichÃ© si lâ€™application nâ€™a encore reÃ§u aucun contenu.

---------
