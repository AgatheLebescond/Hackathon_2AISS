import os
import streamlit as st

from ingestion.extractor import extract_text_from_file
from ingestion.cleaner import clean_text
from ingestion.processing.splitter import split_text
from ingestion.processing.embedder import embed_chunks
from ingestion.processing.indexer import build_faiss_index, search_top_k
from ingestion.processing.summarizer import generate_summary
from ingestion.processing.sentiment_analyzer import analyze_sentiment
from ingestion.processing.wordcloud_generator import generate_wordcloud
from ingestion.newsapi_fetcher import fetch_article_from_url

st.set_page_config(page_title="AI RÃ©sumeur", layout="wide")

st.title("ğŸ§  RÃ©sumeur intelligent d'articles & PDF")

# --- UPLOAD FICHIER OU URL ---
tab1, tab2 = st.tabs(["ğŸ“„ Fichier PDF/DOCX", "ğŸŒ Article en ligne"])

with tab1:
    uploaded_file = st.file_uploader("TÃ©lÃ©verser un fichier", type=["pdf", "docx"])
    if uploaded_file:
        doc_id = os.path.splitext(uploaded_file.name)[0]
        filepath = f"data/uploads/{uploaded_file.name}"
        with open(filepath, "wb") as f:
            f.write(uploaded_file.read())
        raw_text = extract_text_from_file(filepath)

with tab2:
    url = st.text_input("Coller lâ€™URL dâ€™un article dâ€™actualitÃ©")
    if st.button("Extraire lâ€™article depuis lâ€™URL") and url:
        article_data = fetch_article_from_url(url)
        raw_text = article_data["text"]
        metadata = article_data["metadata"]
        doc_id = metadata.get("title", "article")

# --- TRAITEMENT ---
if 'raw_text' in locals():
    cleaned = clean_text(raw_text)
    chunks = split_text(cleaned)
    embeddings = embed_chunks(chunks)
    index = build_faiss_index(embeddings)

    # RÃ©sumÃ© automatique
    summary = generate_summary(chunks)
    st.markdown("### ğŸ“ RÃ©sumÃ© gÃ©nÃ©rÃ©")
    st.success(summary)

    # Analyse de sentiment
    score, label = analyze_sentiment(summary)
    st.markdown("### ğŸ“Š Analyse de sentiment")
    st.write(f"**TonalitÃ© dÃ©tectÃ©e :** {label} (score : {score:.2f})")

    # Nuage de mots
    wordcloud_path = generate_wordcloud(cleaned, doc_id)
    st.markdown("### â˜ï¸ Nuage de mots")
    st.image(wordcloud_path, caption="Vocabulaire dominant")

    # MÃ©tadonnÃ©es
    if 'metadata' in locals():
        st.markdown("### â„¹ï¸ MÃ©tadonnÃ©es de lâ€™article")
        st.write(f"ğŸ“° Source : **{metadata.get('source', 'N/A')}**")
        st.write(f"ğŸ•“ Date : {metadata.get('publishedAt', 'N/A')}")
        if metadata.get("image_url"):
            st.image(metadata["image_url"], caption="Image dâ€™illustration")

    # Exports
    st.download_button("ğŸ’¾ TÃ©lÃ©charger le rÃ©sumÃ©", summary, file_name=f"{doc_id}_resume.txt")
    st.download_button("ğŸ’¾ TÃ©lÃ©charger le nuage de mots", open(wordcloud_path, "rb"), file_name=f"{doc_id}_wordcloud.png")

else:
    st.info("Uploade un fichier ou entre une URL pour commencer.")



